from __future__ import annotations

import logging
import threading
from typing import Generator, List, Tuple

import faiss  # type: ignore
import torch  # type: ignore
from sentence_transformers import CrossEncoder, SentenceTransformer
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextIteratorStreamer,
    pipeline,
)
from peft import PeftModel  # type: ignore

from ..config import CFG
from ..data.io import load_index, load_metadata
from .guards import too_similar

logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    level=logging.DEBUG if CFG.debug else logging.INFO,
    datefmt="%H:%M:%S",
)


# ---------------------------------------------------------------------------
# resource initialisation
# ---------------------------------------------------------------------------

def _boot() -> tuple[
    faiss.Index,
    list[str],
    list[str],
    SentenceTransformer,
    CrossEncoder,
    pipeline,
]:
    logger.info("Loading FAISS index and metadata ‚Ä¶")
    index = load_index(CFG.index_path)
    texts, urls = load_metadata(CFG.meta_path)

    device = "cuda" if CFG.cuda else "cpu"
    logger.info("Initialising embedding & re‚Äërank models ‚Ä¶")
    embedder = SentenceTransformer(CFG.embed_model, device=device)
    reranker = CrossEncoder(CFG.rerank_model, device=device)

    logger.info("Loading LoRA‚Äëmerged generator ‚Ä¶")
    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(CFG.base_model, use_fast=True)
    base = AutoModelForCausalLM.from_pretrained(
        CFG.base_model,
        quantization_config=quant_cfg,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    lora = PeftModel.from_pretrained(base, CFG.lora_dir)
    merged = lora.merge_and_unload()

    chat_pipe = pipeline(
        "text-generation",
        model=merged,
        tokenizer=tokenizer,
        device_map="auto",
        max_new_tokens=CFG.max_new_tokens,
        do_sample=False,
    )
    logger.info("Boot complete.")
    return index, texts, urls, embedder, reranker, chat_pipe


INDEX, TEXTS, URLS, EMBEDDER, RERANKER, CHAT = _boot()


# ---------------------------------------------------------------------------
# retrieval
# ---------------------------------------------------------------------------

def retrieve_unique(query: str) -> List[Tuple[float, str, str]]:
    logger.debug("üîç Query: %s", query)

    q_vec = EMBEDDER.encode(query, normalize_embeddings=True).astype("float32")[
        None, :
    ]
    _d, idx = INDEX.search(q_vec, 100)

    candidates = [(TEXTS[i], URLS[i]) for i in idx[0]]
    raw_scores = RERANKER.predict([(query, t) for t, _ in candidates])

    best: dict[str, Tuple[float, str]] = {}
    for score, (text, url) in zip(raw_scores, candidates):
        if score < CFG.score_min:
            continue
        best[url] = max(best.get(url, (0, "")), (score, text))

    uniques = sorted(
        ((s, t, u) for u, (s, t) in best.items()),
        key=lambda x: x[0],
        reverse=True,
    )[: CFG.top_k]

    logger.debug("Retrieved %d unique passages.", len(uniques))
    return uniques


# ---------------------------------------------------------------------------
# generation
# ---------------------------------------------------------------------------

def answer_stream(
    history: List[dict[str, str]],
) -> Generator[Tuple[List[dict[str, str]], List[dict[str, str]]], None, None]:
    """Stream a RAG-grounded answer for the Gradio UI."""
    user_q = history[-1]["content"]

    passages = retrieve_unique(user_q)
    if not passages:
        history.append(
            {"role": "assistant", "content": "Sorry, I couldn‚Äôt find anything relevant."}
        )
        yield history, history
        return

    src_block = "\n\n".join(
        f"[{i+1}] {url}\n{text}" for i, (_s, text, url) in enumerate(passages)
    )
    prompt = (
        "Answer the *single* question below using only the listed sources. "
        "Do not add additional questions, FAQs, or headings. "
        "Cite each fact like [1].\n\n"
        f"{src_block}\n\nQ: {user_q}\nA:"
    )

    tok = CHAT.tokenizer
    model = CHAT.model
    inputs = tok(prompt, return_tensors="pt").to(model.device)
    if "attention_mask" not in inputs:
        inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

    streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=False)
    threading.Thread(
        target=model.generate,
        kwargs=dict(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=CFG.max_new_tokens,
            do_sample=False,
            streamer=streamer,
        ),
        daemon=True,
    ).start()

    history.append({"role": "assistant", "content": ""})
    partial = ""
    for token in streamer:
        partial += token
        history[-1]["content"] = partial
        yield history, history

    final_answer = history[-1]["content"].strip()

    if too_similar(final_answer, passages, EMBEDDER, CFG.sim_threshold):
        final_answer = "Sorry, the answer is too similar to the source material."

    sources_md = "\n".join(
        f"[{i+1}] {url}" for i, (_s, _t, url) in enumerate(passages)
    )
    history[-1]["content"] = f"{final_answer}\n\n**Sources**\n{sources_md}"
    yield history, history
