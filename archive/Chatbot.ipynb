{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfa7e85-1f34-4bf9-b19d-b60743f4542b",
   "metadata": {},
   "source": [
    "# Visit Grand Junction – End‑to‑End RAG Notebook (GPU, LoRA, UX Extras)\n",
    "This single Jupyter notebook now contains **all 12 requested upgrades**:\n",
    "\n",
    "1. Incremental recrawl with SHA‑256 hashing  \n",
    "2. Back‑off & retry on fetch failures  \n",
    "3. MIME‑type detection (saves PDFs/Docs separately)  \n",
    "4. Boilerplate removal via **trafilatura**  \n",
    "5. Semantic chunking (sentence/window)  \n",
    "6. Retrieval **+ BGE reranking**  \n",
    "7. Local **Mistral‑7B** with LoRA support (4‑bit)  \n",
    "8. In‑notebook LoRA fine‑tuning on branded Q&A  \n",
    "9. Streamlit front‑end with citation pop‑overs  \n",
    "10. Image carousel placeholder  \n",
    "11. Follow‑up suggestion chips  \n",
    "12. Chat‑history export to PDF\n",
    "\n",
    "Run sections sequentially (0 → 11).  \n",
    "LoRA training needs ~24 GB GPU VRAM or adjust batch/accumulation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6303a9-d508-4ecb-8935-2d8649d07975",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install faiss-cpu aiohttp trafilatura peft sentence-transformers trl accelerate qdrant-client pdfkit reportlab streamlit pymupdf nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26da00-f8c8-4192-b694-fea8b8e435e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio, hashlib, json, mimetypes, os, random, re, time, uuid, xml.etree.ElementTree as ET\n",
    "# from pathlib import Path\n",
    "# from urllib.parse import urljoin, urldefrag, urlparse\n",
    "\n",
    "# import aiohttp, faiss, numpy as np, requests, torch\n",
    "# from bs4 import BeautifulSoup\n",
    "# from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "# from tqdm.notebook import tqdm\n",
    "# from trafilatura import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efe814-f21c-4c32-b996-56acd7f32e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, aiohttp, hashlib, json, mimetypes, os, random, re, time, uuid, textwrap, datetime, xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin, urldefrag\n",
    "import bs4, trafilatura, nltk, requests\n",
    "import torch, faiss, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608366-ecc2-4a71-9bdd-bebd250970a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "for res in (\"punkt\", \"punkt_tab\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{res}\")\n",
    "    except LookupError:\n",
    "        nltk.download(res, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129e84d-1aa5-41ba-ac82-d693506517c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL              = \"https://www.visitgrandjunction.com\"\n",
    "ADDITIONAL_DOMAINS    = [\"campaign-archive.com\", \"mailchi.mp\"]\n",
    "DATA_DIR_TXT          = Path(\"data/html_txt\");    DATA_DIR_TXT.mkdir(parents=True, exist_ok=True)\n",
    "RAW_HTML_DIR          = Path(\"data/raw_html\");    RAW_HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NO_TEXT_HTML_DIR      = Path(\"data/html_no_text\");NO_TEXT_HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MIME_DIR              = Path(\"data/mime\");        MIME_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CRAWL_DELAY           = 0.5\n",
    "N_WORKERS             = 10\n",
    "MAX_RETRIES           = 5\n",
    "BACKOFF_FACTOR        = 1.5\n",
    "EMBED_MODEL_NAME      = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RERANKER_NAME         = \"BAAI/bge-reranker-base\"\n",
    "LLM_BASE              = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "LORA_ADAPTER_OUT      = Path(\"lora-vgj-checkpoint\")\n",
    "MANUAL_QA_JL          = Path(\"vgj_lora_dataset.jsonl\")\n",
    "AUTO_QA_JL            = Path(\"vgj_auto_dataset.jsonl\")\n",
    "COMBINED_QA_JL        = Path(\"vgj_combined.jsonl\")\n",
    "INDEX_PATH            = Path(\"faiss.index\")\n",
    "CHUNK_TOKENS          = 200\n",
    "OVERLAP_TOKENS        = 40\n",
    "DEVICE                = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HASH_RECORDS          = Path(\"data/hashes.json\")\n",
    "DEBUG                 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca4d68-2706-4466-b0f3-df1a5fff39f5",
   "metadata": {},
   "source": [
    "## 1  Helper functions (robots, hashing, sitemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa5aa7-ca47-4c46-a7ed-51bedb73b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbg(*msg): \n",
    "    if DEBUG: print(*msg)\n",
    "\n",
    "def robots_disallow(domain):\n",
    "    try:\n",
    "        txt = requests.get(f\"https://{domain}/robots.txt\", timeout=10).text.lower()\n",
    "        return [l.split(\":\",1)[1].strip() for l in txt.splitlines() if l.startswith(\"disallow\")]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def internal_set(base, extra):\n",
    "    return {urlparse(base).netloc, *extra}\n",
    "\n",
    "def allowed(url, nets, dis_map):\n",
    "    p = urlparse(url)\n",
    "    if p.scheme not in {\"http\", \"https\"}:\n",
    "        return False\n",
    "\n",
    "    # accept if the host ends with any entry in nets\n",
    "    if not any(p.netloc == n or p.netloc.endswith(\".\" + n) for n in nets):\n",
    "        return False\n",
    "\n",
    "    # robots.txt disallows\n",
    "    return not any(url.startswith(path) for path in dis_map.get(p.netloc, []))\n",
    "\n",
    "async def sitemap_seed(base, nets):\n",
    "    try:\n",
    "        r = requests.get(f\"{base}/sitemap.xml\", timeout=15); r.raise_for_status()\n",
    "        locs = re.findall(r\"<loc>(.*?)</loc>\", r.text)\n",
    "        urls = [u for u in locs if allowed(u, nets, {})]\n",
    "        dbg(f\"Sitemap OK – {len(urls)} internal URLs\")\n",
    "        return urls or [base]\n",
    "    except Exception as e:\n",
    "        dbg(\"Sitemap fetch failed:\", e)\n",
    "        return [base]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d4670-8a66-4cad-a230-83975995b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha256(b): return hashlib.sha256(b).hexdigest()\n",
    "HASH_DB = json.loads(HASH_RECORDS.read_text()) if HASH_RECORDS.exists() else {}\n",
    "def upsert_hash(url, h):\n",
    "    HASH_DB[url] = h\n",
    "    HASH_RECORDS.write_text(json.dumps(HASH_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82006d0-eea9-4e6a-93a9-fe644b51af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, delay): self.delay, self.next_ts, self.lock = delay, 0, asyncio.Lock()\n",
    "    async def __aenter__(self):\n",
    "        async with self.lock:\n",
    "            await asyncio.sleep(max(0, self.next_ts - time.time()))\n",
    "            self.next_ts = time.time() + self.delay\n",
    "    async def __aexit__(self, *_): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edf7f6-0344-4259-a21a-1d58ffdc41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    # Windows Chrome\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/125.0.0.0 Safari/537.36\",\n",
    "\n",
    "    # macOS Safari\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_5) \"\n",
    "    \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "    \"Version/17.4 Safari/605.1.15\",\n",
    "\n",
    "    # Firefox\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) \"\n",
    "    \"Gecko/20100101 Firefox/124.0\",\n",
    "\n",
    "    # Microsoft Edge\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/125.0.0.0 Safari/537.36 Edg/125.0.0.0\",\n",
    "]\n",
    "\n",
    "async def fetch(session, url, rl):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            async with rl:\n",
    "                async with session.get(url, timeout=20) as r:\n",
    "                    r.raise_for_status()\n",
    "                    mime = r.headers.get(\"content-type\",\"text/html\").split(\";\")[0]\n",
    "                    return mime, await r.read()\n",
    "        except Exception:\n",
    "            await asyncio.sleep(BACKOFF_FACTOR ** attempt)\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3c051-03fa-491d-bdc0-799559558b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def crawl(seed):\n",
    "    assert seed, \"Seed list empty – nothing to crawl.\"\n",
    "    nets = internal_set(BASE_URL, ADDITIONAL_DOMAINS)\n",
    "    dis  = {n: robots_disallow(n) for n in nets}\n",
    "    q    = asyncio.Queue();  [q.put_nowait(u) for u in seed]\n",
    "    seen = set()\n",
    "\n",
    "    ua = random.choice(USER_AGENTS)\n",
    "    headers = {\n",
    "        \"User-Agent\":        ua,\n",
    "        \"Accept\":            \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "                             \"image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\":   \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\":   \"gzip, deflate, br\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Connection\":        \"keep-alive\",\n",
    "    }\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        tasks = [asyncio.create_task(\n",
    "                    worker(f\"w{i}\", i, session, q, seen, CRAWL_DELAY, nets, dis))\n",
    "                 for i in range(N_WORKERS)]\n",
    "        await q.join()\n",
    "        for t in tasks: t.cancel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe37663-86d6-4d1c-b6bf-3e5ff9d23968",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def worker(name, idx, session, q, seen, delay, nets, dis):\n",
    "    rl  = RateLimiter(delay)\n",
    "    bar = tqdm(total=0, position=idx+1, desc=name, unit=\"pg\", leave=True)\n",
    "\n",
    "    while True:\n",
    "        url = await q.get(); q.task_done()\n",
    "        if url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "        bar.set_description(f\"{name} {url}\")\n",
    "\n",
    "        mime, body = await fetch(session, url, rl)\n",
    "        if not body:\n",
    "            bar.update(); continue\n",
    "\n",
    "        uid = hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "        # ───── Non-HTML assets ─────\n",
    "        if mime != \"text/html\":\n",
    "            ext = mimetypes.guess_extension(mime) or \".bin\"\n",
    "            (MIME_DIR / f\"{uid}{ext}\").write_bytes(body)\n",
    "            bar.update(); continue\n",
    "\n",
    "        # Always parse links from *any* HTML page\n",
    "        soup = bs4.BeautifulSoup(body, \"lxml\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            link, _ = urldefrag(urljoin(url, a[\"href\"]))\n",
    "            if allowed(link, nets, dis):\n",
    "                q.put_nowait(link)\n",
    "\n",
    "        # Clean-text extraction to decide where to store the page\n",
    "        text = trafilatura.extract(body) or \"\"\n",
    "        unsupported = \"your browser is not supported for this experience\" in text.lower()\n",
    "\n",
    "        if len(text) < 100 or unsupported:\n",
    "            # HTML with little/no useful text\n",
    "            (NO_TEXT_HTML_DIR / f\"{uid}.html\").write_bytes(body)\n",
    "            bar.update(); continue\n",
    "\n",
    "        # “Good” page: keep everything\n",
    "        (RAW_HTML_DIR  / f\"{uid}.html\").write_bytes(body)\n",
    "        (DATA_DIR_TXT  / f\"{uid}.txt\").write_text(text)\n",
    "        (DATA_DIR_TXT  / f\"{uid}.url\").write_text(url)\n",
    "        bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e7275-a07d-43de-b433-8076efc7d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG      = True\n",
    "seed_urls  = await sitemap_seed(BASE_URL, internal_set(BASE_URL, ADDITIONAL_DOMAINS))\n",
    "await crawl(seed_urls)\n",
    "DEBUG      = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890d8ac-c985-4401-82b0-1edf9714af02",
   "metadata": {},
   "source": [
    "## 3  Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b55cd-0a15-4370-9161-fd1113a3c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Chunk helper  (sentence-aware, overlap aware)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def chunks(text: str, max_tok: int = CHUNK_TOKENS, ov: int = OVERLAP_TOKENS):\n",
    "    \"\"\"\n",
    "    Yield overlapping chunks of <= max_tok words.\n",
    "    Overlap = last `ov` sentences of the previous chunk.\n",
    "    \"\"\"\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    buf, cur = [], 0                 # buffer of sentences, current token count\n",
    "\n",
    "    for s in sents:\n",
    "        n = len(s.split())\n",
    "        if cur + n > max_tok and buf:\n",
    "            yield \" \".join(buf)\n",
    "            # keep the *sentences* overlap, not tokens\n",
    "            buf = buf[-ov:] if ov else []\n",
    "            cur = sum(len(t.split()) for t in buf)\n",
    "\n",
    "        buf.append(s)\n",
    "        cur += n\n",
    "\n",
    "    if buf:\n",
    "        yield \" \".join(buf)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2.  Build / refresh FAISS index\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from tqdm.auto import tqdm\n",
    "import faiss, json, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TXT_DIR   = DATA_DIR_TXT                 # data/html_txt/\n",
    "META_JSON = Path(\"meta.jsonl\")           # one JSON per line\n",
    "\n",
    "embedder  = SentenceTransformer(EMBED_MODEL_NAME, device=DEVICE)\n",
    "\n",
    "index = None                # lazy-init once we know the dim\n",
    "meta_f = META_JSON.open(\"w\")\n",
    "\n",
    "EXCLUDE_PREFIX = \"https://www.visitgrandjunction.com/blog/all-posts\"\n",
    "\n",
    "# stream through all .txt files\n",
    "files = sorted(TXT_DIR.glob(\"*.txt\"))\n",
    "for f in tqdm(files, desc=\"chunk->embed->index\", unit=\"doc\"):\n",
    "    url = (f.parent / f\"{f.stem}.url\").read_text().strip()\n",
    "\n",
    "    # ─── skip anything under /blog/all-posts ───\n",
    "    if url.startswith(EXCLUDE_PREFIX):\n",
    "        continue\n",
    "    \n",
    "    txt = f.read_text()\n",
    "\n",
    "    for chunk in chunks(txt):\n",
    "        vec = embedder.encode([chunk],\n",
    "                              convert_to_numpy=True,\n",
    "                              normalize_embeddings=True)[0]\n",
    "\n",
    "        if index is None:                # first vector → build index shell\n",
    "            index = faiss.IndexFlatIP(vec.shape[0])\n",
    "\n",
    "        index.add(vec.reshape(1, -1))\n",
    "\n",
    "        # keep meta aligned with vector order\n",
    "        meta_f.write(json.dumps({\"url\": url, \"text\": chunk}) + \"\\n\")\n",
    "\n",
    "meta_f.close()\n",
    "faiss.write_index(index, str(INDEX_PATH))   # ← convert Path → str\n",
    "print(f\"Indexed {index.ntotal:,} chunks → {INDEX_PATH}\")\n",
    "print(f\"Meta written           → {META_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a3584-373d-4139-b672-81868fc17d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3.  Reranker + retrieval helper\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import itertools\n",
    "\n",
    "# reload meta into memory (URLs are small)\n",
    "meta_records = [json.loads(l) for l in META_JSON.open()]\n",
    "\n",
    "reranker = CrossEncoder(RERANKER_NAME, device=DEVICE)\n",
    "\n",
    "def retrieve(query: str, k: int = 20, n: int = 5):\n",
    "    \"\"\"\n",
    "    1) semantic search k candidates with SBERT+FAISS\n",
    "    2) Cross-Encoder rerank → top-n paragraphs\n",
    "    Returns list of (paragraph, url).\n",
    "    \"\"\"\n",
    "    qvec = embedder.encode([query],\n",
    "                           convert_to_numpy=True,\n",
    "                           normalize_embeddings=True)\n",
    "    D, I = index.search(qvec, k)\n",
    "    if not len(I[0]):\n",
    "        return []\n",
    "\n",
    "    # candidate texts / meta\n",
    "    cand_texts = [meta_records[i][\"text\"] for i in I[0]]\n",
    "    cand_urls  = [meta_records[i][\"url\"]  for i in I[0]]\n",
    "\n",
    "    scores = reranker.predict(list(zip(itertools.repeat(query), cand_texts)))\n",
    "    best   = np.argsort(scores)[::-1][:n]\n",
    "\n",
    "    return [(cand_texts[i], cand_urls[i]) for i in best]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97119d-8b08-44d6-a104-88648f4764e1",
   "metadata": {},
   "source": [
    "## 5  LoRA‑ready model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4a05b-efba-49f8-b7c2-a11fd6b3e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\n",
    "# from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "# def load_lora_model(ckpt=None):\n",
    "#     base=AutoModelForCausalLM.from_pretrained(BASE_LLM,load_in_4bit=True,device_map=\"auto\",torch_dtype=torch.float16)\n",
    "#     base=prepare_model_for_kbit_training(base)\n",
    "#     if ckpt and Path(ckpt).exists():\n",
    "#         print(\"Loading LoRA weights…\"); base.load_adapter(ckpt)\n",
    "#     tok=AutoTokenizer.from_pretrained(BASE_LLM)\n",
    "#     return base,tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a36b05-eda2-413e-9723-64338fdd9179",
   "metadata": {},
   "source": [
    "## 6  Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe5647-6192-4daa-a102-0b2264b9c1b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ───────────────────────── configuration ─────────────────────────\n",
    "LLM_NAME         = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "PARA_MAX         = 3\n",
    "ANSWER_TOK_CAP   = 220\n",
    "\n",
    "TXT_DIR          = Path(\"data/html_txt\")   # crawled .txt files\n",
    "RAW_HTML_DIR     = Path(\"data/raw_html\")   # raw HTML files\n",
    "\n",
    "MANUAL_QA_JL     = Path(\"vgj_lora_dataset.jsonl\")\n",
    "AUTO_QA_JL       = Path(\"vgj_auto_dataset.jsonl\")\n",
    "COMBINED_QA_JL   = Path(\"vgj_combined.jsonl\")\n",
    "\n",
    "# ───────── load base model once (4-bit) ─────────\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch, json, re, bs4, nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "quant_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(LLM_NAME, use_fast=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_NAME,\n",
    "    quantization_config=quant_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# ───────── helper: generate a question  ─────────\n",
    "def gen_question(passage: str) -> str:\n",
    "    sys = (\"You are a helpful travel assistant. Read the PASSAGE and invent one \"\n",
    "           \"concise, natural-sounding traveler question that could be answered \"\n",
    "           \"by the same passage. Return ONLY the question text.\")\n",
    "    prompt = (f\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n\"\n",
    "              f\"PASSAGE:\\n'''{passage}'''\\n[/INST]\")\n",
    "    ids  = tok(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "    with torch.no_grad():\n",
    "        out = llm.generate(**ids, max_new_tokens=40,\n",
    "                           pad_token_id=tok.eos_token_id)[0]\n",
    "    q = tok.decode(out[ids.input_ids.shape[-1]:],\n",
    "                   skip_special_tokens=True).strip()\n",
    "    return q if q.endswith(\"?\") else q + \"?\"\n",
    "\n",
    "DEBUG = True   # turn off for production\n",
    "\n",
    "# ───────── boiler-plate detector ─────────\n",
    "BOILER_PAT = re.compile(\n",
    "    r\"(click here|minute read|photo credit|browser is not supported)\",\n",
    "    flags=re.I,\n",
    ")\n",
    "\n",
    "# ───────── build auto Q-A (drop dirty rows) ─────────\n",
    "collapse = lambda s: re.sub(r\"\\s+\", \" \", s).strip()\n",
    "auto_examples, skipped = [], 0\n",
    "\n",
    "for txt_f in tqdm(sorted(TXT_DIR.glob(\"*.txt\")), desc=\"auto-QA\", unit=\"page\"):\n",
    "    url  = txt_f.with_suffix(\".url\").read_text().strip()\n",
    "    html = (RAW_HTML_DIR / f\"{txt_f.stem}.html\").read_text()\n",
    "    soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    paras = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paras = [p for p in paras if len(p.split()) > 25][:PARA_MAX]\n",
    "    if not paras:\n",
    "        continue\n",
    "\n",
    "    passage  = \"\\n\\n\".join(paras)\n",
    "    question = gen_question(passage)\n",
    "\n",
    "    # cap answer length\n",
    "    words, answer_words = 0, []\n",
    "    for p in paras:\n",
    "        if words + len(p.split()) > ANSWER_TOK_CAP:\n",
    "            break\n",
    "        answer_words.extend(p.split()); words += len(p.split())\n",
    "    answer = \" \".join(answer_words) or paras[0]\n",
    "\n",
    "    # ── skip if answer contains boiler-plate ──\n",
    "    if BOILER_PAT.search(answer):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nQ: {question}\\nA: {answer[:120]}…\\n\")\n",
    "    auto_examples.append({\"instruction\": question,\n",
    "                          \"input\": \"\",\n",
    "                          \"output\": answer})\n",
    "\n",
    "print(f\"Skipped {skipped} junk excerpts\")\n",
    "\n",
    "# ───────── write JSONL files ─────────\n",
    "with AUTO_QA_JL.open(\"w\") as f:\n",
    "    for ex in auto_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "with COMBINED_QA_JL.open(\"w\") as out:\n",
    "    for src in (MANUAL_QA_JL, AUTO_QA_JL):\n",
    "        if src.exists():\n",
    "            out.writelines(src.open())\n",
    "\n",
    "print(f\"Generated {len(auto_examples):,} clean pairs → {AUTO_QA_JL}\")\n",
    "print(f\"Combined dataset written       → {COMBINED_QA_JL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7725ab14-b746-4525-ae8e-45836535d01f",
   "metadata": {},
   "source": [
    "## 7  In‑notebook LoRA fine‑tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aea3f8-f337-4d4e-97da-e71ca2074fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6cd9acb9b3468e9da43e7a52ee5fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
      "train = 417 rows • eval = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/sd205521/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:856: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 84/260 05:51 < 12:35, 0.23 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.493400</td>\n",
       "      <td>3.641377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.531300</td>\n",
       "      <td>3.602441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.388700</td>\n",
       "      <td>3.523367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.295700</td>\n",
       "      <td>3.401173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.206100</td>\n",
       "      <td>3.237560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.077200</td>\n",
       "      <td>3.041271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.883100</td>\n",
       "      <td>2.831852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.706300</td>\n",
       "      <td>2.636621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.579100</td>\n",
       "      <td>2.496101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.466500</td>\n",
       "      <td>2.430332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.230100</td>\n",
       "      <td>2.397627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.323800</td>\n",
       "      <td>2.366172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.327500</td>\n",
       "      <td>2.324404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.285500</td>\n",
       "      <td>2.275702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.179100</td>\n",
       "      <td>2.229687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.076100</td>\n",
       "      <td>2.190566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.186600</td>\n",
       "      <td>2.156810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.107600</td>\n",
       "      <td>2.126807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.993100</td>\n",
       "      <td>2.099509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.057700</td>\n",
       "      <td>2.077657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.042900</td>\n",
       "      <td>2.061119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.049500</td>\n",
       "      <td>2.047581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.055800</td>\n",
       "      <td>2.047581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.978200</td>\n",
       "      <td>2.033851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.040600</td>\n",
       "      <td>2.019806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.848000</td>\n",
       "      <td>2.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.396400</td>\n",
       "      <td>1.991611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.844000</td>\n",
       "      <td>1.977684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.881200</td>\n",
       "      <td>1.965389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.821100</td>\n",
       "      <td>1.952512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.794800</td>\n",
       "      <td>1.940791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.847500</td>\n",
       "      <td>1.929040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.894900</td>\n",
       "      <td>1.917146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.876400</td>\n",
       "      <td>1.905692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.877200</td>\n",
       "      <td>1.894544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.870000</td>\n",
       "      <td>1.883612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.652600</td>\n",
       "      <td>1.872414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.740200</td>\n",
       "      <td>1.861340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.667700</td>\n",
       "      <td>1.851779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.710200</td>\n",
       "      <td>1.843446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.959600</td>\n",
       "      <td>1.835197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.825900</td>\n",
       "      <td>1.827429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.752600</td>\n",
       "      <td>1.820283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.821000</td>\n",
       "      <td>1.814381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.780400</td>\n",
       "      <td>1.809525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.821100</td>\n",
       "      <td>1.804873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.782100</td>\n",
       "      <td>1.800799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.730700</td>\n",
       "      <td>1.796292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.705000</td>\n",
       "      <td>1.791983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.683200</td>\n",
       "      <td>1.787597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.684600</td>\n",
       "      <td>1.782949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>1.778620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.711000</td>\n",
       "      <td>1.774674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.301900</td>\n",
       "      <td>1.771303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.694000</td>\n",
       "      <td>1.768856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.619900</td>\n",
       "      <td>1.767456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.786200</td>\n",
       "      <td>1.764680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.651500</td>\n",
       "      <td>1.761583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.571800</td>\n",
       "      <td>1.759672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.657500</td>\n",
       "      <td>1.758704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.622200</td>\n",
       "      <td>1.759161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.606100</td>\n",
       "      <td>1.756336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.563200</td>\n",
       "      <td>1.751922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.592000</td>\n",
       "      <td>1.747792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.635800</td>\n",
       "      <td>1.744821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.492900</td>\n",
       "      <td>1.741684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.585500</td>\n",
       "      <td>1.738574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.491300</td>\n",
       "      <td>1.737184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.737234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>1.735304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.627500</td>\n",
       "      <td>1.732625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.566900</td>\n",
       "      <td>1.729338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.413100</td>\n",
       "      <td>1.726705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.645100</td>\n",
       "      <td>1.723641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.534800</td>\n",
       "      <td>1.720664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.489800</td>\n",
       "      <td>1.716953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.611500</td>\n",
       "      <td>1.713161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.417200</td>\n",
       "      <td>1.709141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.474500</td>\n",
       "      <td>1.705122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.538300</td>\n",
       "      <td>1.700887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.793200</td>\n",
       "      <td>1.698552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.699350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.417000</td>\n",
       "      <td>1.703315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.482300</td>\n",
       "      <td>1.708818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA adapter + tokenizer saved to → lora-vgj-checkpoint\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────── 0. house-keeping  ───────────────────────\n",
    "from pathlib import Path\n",
    "import torch, gc, os\n",
    "gc.collect(); torch.cuda.empty_cache()          # clear VRAM\n",
    "\n",
    "# ───────────────────── 1. paths & knobs  ────────────────────────\n",
    "BASE_MODEL      = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "COMBINED_QA_JL  = \"vgj_auto_dataset.jsonl\"          # built earlier\n",
    "CHECKPOINT_DIR  = \"lora-vgj-checkpoint\"\n",
    "\n",
    "# LoRA hyper-params\n",
    "LORA_R          = 16\n",
    "LORA_ALPHA      = 32\n",
    "LORA_DROPOUT    = 0.05\n",
    "\n",
    "# training hyper-params\n",
    "BATCH_PER_GPU   = 4          # ← tweak 3\n",
    "GRAD_ACC_STEPS  = 4\n",
    "LOG_STEPS       = 1           # ← tweak 1\n",
    "EVAL_STEPS      = 1          # ← tweak 2\n",
    "PATIENCE   = 3           # stop after 3 stagnant evals\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 2e-4\n",
    "\n",
    "# ───────────────────── 2. tokenizer & base model  ───────────────\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit               = True,\n",
    "    bnb_4bit_quant_type        = \"nf4\",\n",
    "    bnb_4bit_compute_dtype     = torch.float16,\n",
    "    bnb_4bit_use_double_quant  = True\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "tok.pad_token = tok.eos_token           # needed for packed batches\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config = bnb_cfg,\n",
    "    device_map          = {\"\": 0},      # push everything to cuda:0\n",
    "    torch_dtype         = torch.float16\n",
    ")\n",
    "\n",
    "# ───────────────────── 3. wrap with LoRA  ───────────────────────\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r              = LORA_R,\n",
    "    lora_alpha     = LORA_ALPHA,\n",
    "    lora_dropout   = LORA_DROPOUT,\n",
    "    bias           = \"none\",\n",
    "    task_type      = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.print_trainable_parameters()      # sanity check\n",
    "\n",
    "# ───────────────────── 4. dataset  → chat template  ─────────────\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split   # only to shuffle indices\n",
    "\n",
    "def to_chat(example):\n",
    "    user = example[\"instruction\"].strip()\n",
    "    if example[\"input\"]:\n",
    "        user += \"\\n\" + example[\"input\"].strip()\n",
    "    return {\n",
    "        \"text\": (\n",
    "            f\"<s>[INST] {user} [/INST] \"\n",
    "            f\"{example['output'].strip()} </s>\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "# load + template\n",
    "dataset = (load_dataset(\"json\",\n",
    "                        data_files=COMBINED_QA_JL,\n",
    "                        split=\"train\")\n",
    "           .map(to_chat,\n",
    "                remove_columns=[\"instruction\", \"input\", \"output\"]))\n",
    "\n",
    "# 90 / 10 random split   (seed-stable)\n",
    "train_idx, eval_idx = train_test_split(\n",
    "    list(range(len(dataset))), test_size=0.1, random_state=42\n",
    ")\n",
    "train_set = dataset.select(train_idx)\n",
    "eval_set  = dataset.select(eval_idx)\n",
    "\n",
    "print(\"train =\", len(train_set), \"rows • eval =\", len(eval_set))\n",
    "\n",
    "# ───────────────────── 5. training args & trainer  ──────────────\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from trl import SFTTrainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir                   = CHECKPOINT_DIR,\n",
    "    per_device_train_batch_size  = BATCH_PER_GPU,\n",
    "    gradient_accumulation_steps  = GRAD_ACC_STEPS,\n",
    "    num_train_epochs             = EPOCHS,\n",
    "    learning_rate                = LR,\n",
    "    lr_scheduler_type            = \"cosine\",\n",
    "    warmup_ratio                 = 0.03,\n",
    "    logging_steps                = LOG_STEPS,\n",
    "    eval_strategy                = \"steps\",   # <─ ADD THIS\n",
    "    eval_steps                   = EVAL_STEPS,\n",
    "    load_best_model_at_end       = True,        # ← keep best epoch\n",
    "    metric_for_best_model        = \"eval_loss\",\n",
    "    greater_is_better            = False,\n",
    "    save_strategy                = \"steps\",\n",
    "    fp16                         = True,\n",
    "    report_to                    = [],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    args          = train_args,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset  = eval_set,\n",
    "    callbacks     = [EarlyStoppingCallback(\n",
    "                        early_stopping_patience=PATIENCE,\n",
    "                        early_stopping_threshold=0.0)]\n",
    ")\n",
    "\n",
    "\n",
    "# ───────────────────── 6. launch fine-tuning  ───────────────────\n",
    "trainer.train()\n",
    "model.save_pretrained(CHECKPOINT_DIR)\n",
    "tok.save_pretrained(CHECKPOINT_DIR)\n",
    "\n",
    "print(f\"\\nLoRA adapter + tokenizer saved to → {CHECKPOINT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e66dc1-d972-4a57-a398-17433120064d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (HuggingFace)",
   "language": "python",
   "name": "gj-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
